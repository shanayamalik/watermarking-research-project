%% LaTeX Template for ISIT 2025
%%
%% by Stefan M. Moser, October 2017
%% (with minor modifications by Tobias Koch, November 2023 and Michèle Wigger, November 2024)
%% 
%% derived from bare_conf.tex, V1.4a, 2014/09/17, by Michael Shell
%% for use with IEEEtran.cls version 1.8b or later
%%
%% Support sites for IEEEtran.cls:
%%
%% http://www.michaelshell.org/tex/ieeetran/
%% http://moser-isi.ethz.ch/manuals.html#eqlatex
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%%

%% 0. Abstract
%% 1. Introduction (detailed problem motivations)
%% 2. Classic Watermarking
%% 3. Stable Diffusion Watermarking (Tree-ring, PRC, T2S)
%% 4. SOTA Video Watermarking
%% 5. Evaluations
%% 6. Different Latent Position Watermarking

\documentclass[onecolumn]{IEEEtran}

%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}

%%%%%%
%% Packages:
%% Some useful packages (and compatibility issues with the IEEE format)
%% are pointed out at the very end of this template source file (they are 
%% taken verbatim out of bare_conf.tex by Michael Shell).
%
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{ifthen}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{amssymb}
\usepackage{mathrsfs} 
\usepackage{float}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
                             % with IEEE Xplore (see bare_conf.tex)

%% Please note that the amsthm package must not be loaded with
%% IEEEtran.cls because IEEEtran provides its own versions of
%% theorems. Also note that IEEEXplore does not accepts submissions
%% with hyperlinks, i.e., hyperref cannot be used.

\newtheorem{definition}{Definition}

\interdisplaylinepenalty=2500 % As explained in bare_conf.tex


% ------------------------------------------------------------
\begin{document}
\title{On Spatial-Temporal Robust Watermarking With Pseudo Random Codes}
% \title{Identifying AI Content With Imperceptible Watermarks}

% %%% Single author, or several authors with same affiliation:
% \author{%
%  \IEEEauthorblockN{Author 1 and Author 2}
% \IEEEauthorblockA{Department of Statistics and Data Science\\
%                    University 1\\
 %                   City 1\\
  %                  Email: author1@university1.edu}% }


%%% Several authors with up to three affiliations:
\author{%
  \IEEEauthorblockN{Shai Dickman},
  \IEEEauthorblockN{Shanaya Malik},
  \IEEEauthorblockN{Alex Tong},
  \IEEEauthorblockN{Esha Garg}
  \begin{center}
    \today
  \end{center}
}




\maketitle

%%% SHAI %%%
\begin{abstract}
The introduction of generative models has ushered in an era of eerily realistic synthetic content across a broad range of media such as images, audio, and language. Although the capabilities of these models are certainly impressive, the rapid adaptation raise serious concerns about misinformation and copyright abuse. As a result, it is crucial to have reliable methods for identifying artificially generated content. Generative watermarking addresses this challenge by embedding a statistical signal directly into the model's sampling process, offering superior robustness and undetectability compared to classical post-hoc methods. 
% Watermarking addresses this need by embedding a statistical signal during generation that indicates AI origin. Unlike classical watermarking, which applies it post hoc, generative models allow such signals to be integrated directly into the sampling process, enabling stronger robustness and improved undetectability.
% Although the capabilities of these models are certainly impressive, it is important to reflect on their future implications. These models pose significant societal risks, such as enabling the spread of misinformation and making copyright enforcement more difficult. Thus, it is crucial to have reliable methods for identifying artificially generated content, such as watermarking. The goal of a watermark is to embed a strong statistical indicator that content originated from an AI system. This problem is distinct from classical watermarking because the generative processes inherent to AI models can be used to embed watermarks in less detectable and more robust ways. 
However, while watermarking has been studied extensively for language and image models, video generation models remain relatively underexplored despite their growing presence on social media platforms. The temporal aspect of videos, in particular, introduces new vulnerabilities—such as clipping and frame dropping-that complicate detection. In this report, we extend the PRC watermark \citep{gunn2025undetectable} to the video domain, specifically analyzing its robustness against temporal attacks and characterizing the fundamental trade-offs between capacity, imperceptibility, robustness, and undetectability.
\end{abstract}

\section{Introduction}
The rapid proliferation of generative artificial intelligence has fundamentally transformed the digital media landscape. While initial breakthroughs focused on text and images, recent advancements have enabled the creation of high-fidelity synthetic video content that is increasingly becoming indistinguishable from real videos. Proprietary and open sourced models alike have demonstrated the ability to generate complex scenes with eerily realistic temporal consistency and camera motion, which will only improve as time goes on. However, this democratized capability comes with significant societal risks. The ease of generating hyper-realistic deepfakes threatens to accelerate the spread of misinformation and complicate copyright enforcement. Consequently, establishing reliable provenance has become an urgent technical priority. 

To address these risks, a new paradigm of watermark has emerged: generative watermarking. Instead of embedding watermarks post hoc, generative watermarking seeks to embed a statistical signal directly into the video creation process by treating it as a communication channel, creating more robust and imperceptible watermarks that are more persistent compared to classical schemes.

While watermarking static images has been well studied in the past few years, with relatively robust solutions for different types of spatial attacks, video watermarking remains significantly underexplored due to its novelty. It is important to note that videos are not just a sequence of independent images, but rather possess a temporal dimension that introduces a new class of adversarial attacks. In real-world pipelines, videos are frequently subjected to \textit{temporal desynchronization}: they may be clipped to shorter durations, subjected to frame dropping during streaming (packet loss), or subsampled to lower frame rates. For a watermarking system to be practical, it must be robust to these temporal distortions without requiring the detector to have access to meta-information such as which frames were removed.

In this work, we address the challenge of robust video watermarking by extending the PRC watermark \citep{gunn2025undetectable} to the spatio-temporal domain. Using the Open Sora 1.3 architecture as our testbed, we demonstrate that naive extensions of image-based watermarking schemes fail catastrophically under temporal attacks and that a more careful construction is needed to achieve temporal robustness.

Our contributions are as follows:
\begin{itemize}
\item \textbf{Spatial Robustness Baseline}: We extend the evaluation of the PRC watermark for images under geometric attacks not covered in the original work, establishing a critical baseline for understanding the synchronization sensitivity that subsequently guides our video analysis.
\item \textbf{Evaluation of Natural Spatio-temporal Extension}: We formulate a natural extension of the PRC watermark from 2D spatial latents to 3D spatio-temporal latents. While we validate its imperceptibility using the VBench benchmark, we demonstrate that this naive approach is extremely brittle, failing catastrophically under even minor temporal shifts due to the strict index-dependency of the 3D key.
\item \textbf{Analysis of Temporal Broadcasting & Phase Mismatch}: To address this brittleness, we investigate "Temporal Broadcasting" (redundancy over time). We identify a critical "Phase Mismatch" vulnerability where this strategy still fails under front-clipping attacks. We show this occurs because removing frames shifts the alignment of physical pixels relative to the model's fixed latent compression blocks (typically 4:1), scrambling the embedded signal.
\item \textbf{Modulo-4 Alignment Strategy}: We propose a robust detection protocol that resolves these synchronization issues by cycling through temporal phase shifts during extraction. We show that this strategy recovers the watermark under severe attacks, maintaining detectability even with $>55\%$ video removal or stochastic frame dropping with probability up to $p=0.75$.
\item \textbf{Discovery of the "Anchor Effect"}: We characterize the fundamental operating limits of our method, specifically identifying the "Anchor Effect" under subsampling attacks. We demonstrate that detection in subsampled videos is largely an artifact of the model's initialization, relying strictly on the presence of the very first physical frame to preserve the watermark signal.
\end{itemize}

\section{Background}
% \subsection{Problem Motivations}
% From an information-theoretic perspective, watermarking can be modeled as a communication problem over a noisy channel. We want to guarantee reliable communication of an embedded watermark across a family of adversarial channels, subject to imperceptibility constraints. More specifically, for robustness we seek to minimize the probability of decoding error $P_\epsilon$ over a family of channels $\{C_i\} \subseteq \mathcal{A}$, where $\mathcal{A}$ denotes the set of adversarial channels:
% \[\sup_{C_i \in \mathcal{A}}P_\epsilon(C_i) < \delta\] for some small $\delta$. As we will discuss later, these adversarial channels can include attacks on a sample (image, video, etc.) as well as the watermark extraction process if such a process is imperfect.

% In addition to robustness, watermarking must also satisfy imperceptibility, which can be modeled as a distortion constraint. Unlike classical channel coding, where the goal is to maximize rate, watermarking has an additional constraint: the distortion it introduces must fit within a perceptual budget (hidden from humans):
% \begin{equation}
%     \mathbb{E}[d(X,Y)] \leq D_{max}
% \end{equation}
% where $d(\cdot)$ is the distortion measure, $Y$ is the watermarked signal (image), and $X$ is the original. Overall, this makes watermarking a joint rate–distortion and channel coding problem. In practice, a watermarked image may look completely different from a non-watermarked image for the same random seed; however, what truly matters is that the distributions of $X$ and $Y$ are similar. Thus, the distortion measure $d$ is often a distance metric between distributions rather than between individual watermarked and non-watermarked images.

% \subsection{Background Watermarking Techniques}
Watermarking can be viewed as a communication problem: an encoder embeds a signal into content, an adversary applies a channel transformation (e.g., compression, cropping, re-encoding), and a decoder attempts detection or message recovery. A useful way to organize prior work is therefore not only by algorithmic construction, but by \emph{where} the watermark is embedded (pixel space, transform space, or model latent space) and \emph{which distortions} it is designed to survive.

\paragraph{Why include classical baselines?}
Classical post-hoc watermarking serves as an important reference point for this work. First, it represents the standard setting where a watermark is added \emph{after} content generation, without access to the generative process itself. Second, it exposes a recurring vulnerability that becomes even more pronounced in video: \emph{desynchronization}. Attacks that delete, shift, rotate, or resample content often break coordinate alignment, which is frequently more damaging than additive noise and motivates model-native alternatives.

\paragraph{Post-hoc watermarking (pixel and transform domains).}
We consider representative classical watermarking techniques that operate directly on generated samples:
\begin{itemize}
    \item \textbf{Least Significant Bit (LSB).}
    LSB embeds information by modifying the lowest bit planes of pixel values. While it offers high capacity and can be imperceptible under ideal conditions, it is extremely brittle in practice. In our experiments, LSB survived essentially only lossless operations and failed under typical compression, filtering, and spatial perturbations, confirming its unsuitability for adversarial settings.
    
    \item \textbf{Discrete Fourier Transform (DFT).}
    DFT-based watermarking embeds information in the magnitude or phase of the frequency spectrum. Because the Fourier representation captures more global image structure than raw pixels, DFT exhibits improved robustness to certain post-processing operations and mild geometric transformations. Among classical methods, DFT was the strongest baseline in our evaluation, maintaining higher detection confidence than LSB, though it still fails under severe spatial desynchronization without explicit synchronization mechanisms.
\end{itemize}

\paragraph{Generative watermarking (model-native embedding).}
Recent work moves watermarking upstream into the sampling process of diffusion and flow-matching models by modifying the initial Gaussian latent. This reframes watermarking as a distribution-level property of the generator rather than a fragile post-hoc overlay.
\begin{itemize}
    \item \textbf{Tree-Ring Watermarking.}
    Tree-ring watermarking injects a circular band into the frequency domain of the initial latent, typically concentrating energy in higher frequencies to preserve perceptual quality. It demonstrates strong empirical robustness in image diffusion models \citep{wen2023treeringwatermarksfingerprintsdiffusion}.
    
    \item \textbf{Gaussian-Shading Watermarking.}
    Gaussian-shading biases the initial Gaussian latent toward a key-dependent region while preserving the marginal distribution of individual samples \citep{yang2024gaussianshadingprovableperformancelossless}. Despite its robustness, prior work reports a reduction in sample diversity, highlighting a trade-off between detectability and generative richness \citep{gunn2025undetectable}.
    
    \item \textbf{Pseudorandom Codes (PRC).}
    PRC embeds pseudorandom structure into the initial latent to achieve strong detectability without sacrificing sample diversity \citep{gunn2025undetectable}. A recent extension applies PRC to video by distributing code structure across spatiotemporal latents and using alignment-based detection \citep{hu2025videomarkdistortionfreerobustwatermarking}, but evaluates only mild temporal distortions (e.g., single-frame drops). In this work, we study PRC under \emph{severe} spatial and temporal desynchronization and analyze how latent compression architectures fundamentally constrain robustness.
\end{itemize}

\paragraph{Summary and motivation.}
Classical post-hoc schemes degrade rapidly once the adversarial channel includes strong desynchronization, while generative watermarking gains leverage by embedding signals at the model’s natural initialization point. This distinction motivates our focus on PRC, which we evaluate across aggressive spatial and temporal attacks to characterize its operating limits and recovery strategies in both image and video generation models.

\begin{table}[h]
    \centering
    \begin{threeparttable}
        \begin{tabular}{lccc}
            \toprule
            \textbf{Method} & \textbf{Embedding Domain} & \textbf{Strength} & \textbf{Primary Failure Mode} \\
            \midrule
            LSB & Pixel space & High capacity, simple & Compression / mild filtering \\
            DFT & Frequency space & Better post-processing  robustness & Severe desynchronization \\
            Tree-Ring & Gen.\ latent (freq.) & Strong image robustness & Model / attack specificity \\
            Gaussian-Shading & Gen.\ latent (distribution bias) & Robust initialization & Diversity degradation \\
            PRC & Gen.\ latent (pseudorandom) & Fidelity + detectability & Spatial / temporal misalignment \\
            \bottomrule
        \end{tabular}
    \end{threeparttable}

    \vspace{2mm}
    \caption{\textbf{Watermarking baselines and typical failure modes.}
    The key distinction is post-hoc (pixel or transform domain) versus model-native (latent) embedding.}
    \label{tab:wm_baselines}
\end{table}

% Traditional ad-hoc methods such as LSB and DFT treat a sample image as a static asset. While universally applicable, they are often vulnerable to removal. For instance, a simple Gaussian blur or re-encoding can destroy an LSB watermark. Furthermore, they do not exploit the generative priors of the model. Embedding watermarks directly into the generative process (e.g., within the latent space of a Stable Diffusion model) offers distinct advantages:
% \begin{enumerate}
%     \item \textbf{Deep Integration:} The watermark is "baked" into the image during the reverse diffusion process. This makes the signal an integral part of the image structure rather than a fragile overlay.
%     \item \textbf{Rate-Distortion Optimization:} From an information-theoretic perspective, watermarking is a communication problem over a noisy channel. By embedding in the latent space, we can leverage the model's decoder to minimize perceptual distortion ($E[d(X,Y)] \le D_{max}$) while maximizing the reliability of the signal against the "channel noise" of the diffusion process itself.
%     \item \textbf{Robustness to Generative Attacks:} Watermarks native to the model's latent space are better positioned to survive "purification" attacks (e.g., adding noise and re-generating), as the watermark is consistent with the model's internal representation.
% \end{enumerate}
% Our research investigates the PRC watermark which can be embedded in the generative process. We evaluate how this watermark has been used for images, how it can be extended to videos, and how robust it is to spatial and temporal attacks.

\section{Generative Model Inference$\And$Inversion}
As described earlier, it can be valuable to embed watermarks within the sampling process of a generative model rather than after generation. Most image and video generation models use latent space iterative processes that are initialized with Gaussian noise. A common technique is to watermark this initial Gaussian latent through some detectable transformation. To  extract the watermark, however, there needs to be a way to predict the initial latent from a given sample. Thus, an understanding of the inner workings of these generative models is important. We provide a brief overview of the sampling and inverse sampling methods of Stable Diffusion 1.5 \citep{rombach2021highresolution}, a Diffusion image model, and Open Sora 1.3 \citep{opensora}, a Flow-Matching video model. In the following sections, we investigate how to watermark outputs from these models by using corresponding sampling and inverse sampling algorithms.

\subsection{Stable Diffusion 1.5}
\begin{figure}                 
    \centering
    \includegraphics[width=0.48\linewidth]{images/stablediffusiongen.png}
      \quad\quad
    \includegraphics[width=0.38\linewidth]{images/intermediates.png}
    \caption{Stable Diffusion model architecture \citep{rombach2021highresolution} (left), Stable Diffusion 1.5 example: "Claude Shannon holding a red balloon" (right).}
\end{figure}

The iterative processes that define Diffusion models are called the forward and reverses processes. Figure 1 shows these processes in the Stable Diffusion architecture. Forward diffusion iteratively adds Gaussian noise to a data point $x_0$ to achieve a final noised result $x_T$. Here, $x_0$ is the latent representation of an image, which is derived by a pretrained encoder. There is also a corresponding decoder which converts latents to images for the reverse process. The forward process can be written in a closed form as
\begin{equation}
    x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon, \quad \epsilon\sim\mathcal{N}(0,I)
\end{equation}
where $\bar{\alpha}_t$ is a function of the scheduled variances for the iteratively added noise and the $\epsilon$ term represents the cumulative Gaussian noise. Inference is done via the reverse (or denoising) process which generates a sample latent given an initial Gaussian latent. An example of this process is depicted in Figure 1. Denoising Diffusion Implicit Models (DDIM) is a commonly used process for sampling and is used to sample from Stable Diffusion 1.5 \citep{song2022denoisingdiffusionimplicitmodels, rombach2021highresolution}. At each denoising step, we form a prediction $\epsilon_\theta$ of the cumulative noise added to $x_0$ to get $x_t$ during the forward process. We use this noise prediction to form an estimate $\hat{x}_0^t$ of the initial sample $x_0$. Finally, we predict $x_{t-1}$ using the DDIM update step:
\begin{equation}
    x_{t-1}=\sqrt{\bar{\alpha}_{t-1}}\hat{x}_0^t + \sqrt{1-\bar{\alpha}_{t-1}}\epsilon_\theta(x_t)
\end{equation}


We approximate the inversion of this method in the same way as the Tree-ring watermarking paper as well as several others \citep{wen2023treeringwatermarksfingerprintsdiffusion}. This inversion assumes $x_t-x_{t-1}\approx x_{t+1}-x_t$. It is as follows:
\begin{equation}
    x_{t+1}=\sqrt{\bar{\alpha}_{t+1}}\hat{x}_0^t + \sqrt{1-\bar{\alpha}_{t+1}}\epsilon_\theta(x_t)
\end{equation}
This method is an approximation and will accumulate errors from each additional step.

\subsection{Open Sora 1.3}
Flow-Matching is another generative process with strong parallels to Diffusion model, however formulated in a distinct way. The forward process is a linear deterministic interpolation between the data point $x_0$ and a noise term $\epsilon$ \citep{lipman2023flowmatchinggenerativemodeling}. In Open Sora, $x_0$ is the latent representation of an entire video. Like Stable Diffusion, Open Sora uses an encoder and decoder to map between pixel/temporal space and latent space. It also lets the noise term $\epsilon$ be Gaussian which makes the forward process correspond to a Diffusion forward process. The distinct difference between the latent representation of Open Sora and Stable Diffusion is that Open Sora uses an additional temporal dimension. Also, while Diffusion models are trained to predict the additive noise at a given timestep, Flow-Matching models are trained as vector fields $v_\theta$ that can be integrated via standard ODE solvers to derive samples $\hat{x}_0$ from an initial state of Gaussian noise \citep{lipman2023flowmatchinggenerativemodeling}. These two methods are very similar and in fact the DDIM update step can be shown to have the same structure as Euler's method, a first-order accurate scheme for solving ODEs and a popular method used in Flow-Matching inference \citep{xu2025unveilinversioninvarianceflow}. The Flow-Matching update step is:
\begin{equation}
    x_{n-1}=x_n+(t_n-t_{n-1})v_\theta(x_n;t_n)
\end{equation}


Operating under the same assumption as DDIM inversion that $x_t-x_{t-1}\approx x_{t+1}-x_t$, we use the following inverse step:
\begin{equation}
    x_n=x_{n-1}-(t_n-t_{n-1})v_\theta(x_{n-1};t_n)
\end{equation}


Like DDIM, there are accuracy concerns with this inversion since it is still an approximation. For an exact inversion, we can solve the implicit update step
\begin{equation}
    x_n=x_{n-1}-(t_n-t_{n-1})v_\theta(x_n;t_n)
\end{equation}
but this is computationally more demanding as it will require multiple fixed point iterations. This method has been used for image editing which requires higher accuracy inversion \citep{xu2025unveilinversioninvarianceflow}, however, like with DDIM, we found that an approximate inversion is sufficient for extracting the watermark.

\subsection{Consequences of Approximate Inversions}
It is important to acknowledge an additional challenge of injecting a watermark into the initial latent of a generative model: the inverse process to predict the initial latent can in itself degrade the watermark, even without any attacks on the original sample. The inverse processes for Stable Diffusion and Open Sora, while slightly different, are both approximations and accumulate error.

Let $X$ be the watermarked image, $Y$ be the attacked image, and $f(Y)$ be the predicted noise latent from the attacked image. We can view the errors introduced by $f$ as just additional image attacks. Then $X-Y-f(Y)$ form a Markov Chain. By the Data Processing Inequality, $I(X,f(Y))\leq I(X, Y)$. It follows that the capacity of a watermark extracted from the approximate inverted latent representation $f(Y)$ is upper bounded by the capacity of a watermark extracted directly from $Y$. Thus, if $f$ is not a bijection, we will theoretically have lower watermark capacity by embedding it into the initial Gaussian latent. Even still, generative watermarking is appealing because the initial latent Gaussian distribution creates a great opportunity for elegant schemes with much less impact on visual quality and yet also more robustness than many common ad-hoc methods.

\input{prc-overview}

\section{Experiments}
\subsection{Experiment Setup}
Our experimental design is divided into two phases: spatial robustness and temporal robustness. The first phase establishes a baseline by evaluating the PRC watermark's robustness against spatial desynchronization attacks on images generated by Stable Diffusion 1.5. The second phase, which forms the core of this work, extends the PRC watermark to the temporal domain using Open Sora 1.3 and rigorously evaluates its resilience against video-specific desynchronization attacks.



To build intuition for the video domain, we first examine the analogous effects of spatial desynchronization—specifically cropping and rotation—on watermarked images generated by the Stable Diffusion 1.5 text-to-image model. While these geometric attacks were not discussed in the original PRC paper \citep{gunn2025undetectable}, they serve as critical benchmarks in the broader watermarking literature \citep{wen2023treeringwatermarksfingerprintsdiffusion} and are a necessary precursor to our analysis of video.

\subsection{Stable Diffusion 1.5}
To build intuition for the video domain, we first examine the effects of spatial desynchronization on watermarked images generated by the Stable Diffusion 1.5 text-to-image model. While PRC was originally proposed and evaluated without geometric attacks \citep{gunn2025undetectable}, spatial perturbations such as cropping and rotation are standard robustness benchmarks in classical and generative watermarking \citep{wen2023treeringwatermarksfingerprintsdiffusion}. These attacks serve as direct spatial analogs to temporal clipping and frame dropping in videos.

\paragraph{Attack Setup}
We evaluate masked cropping and rotation attacks that simulate common post-processing operations. For cropping, removed regions are replaced with black pixels while preserving the original image dimensions, isolating the effect of spatial deletion without introducing rescaling artifacts.

\begin{table}[h]
    \centering
    \begin{threeparttable}
        \begin{tabular}{lcc}
            \toprule
            \textbf{Spatial Attack} & \textbf{Decodable Limit} & \textbf{Detectable Limit} \\
            \midrule
            Center crop (masked) & $< 30\%$ removed & $< 50\%$ removed \\
            Corner crop (masked) & $< 25\%$ removed & $< 40\%$ removed \\
            Rotation ($90^\circ$, $270^\circ$) & \multicolumn{2}{c}{\textit{All fail}} \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \small
            \item “Removed” denotes the percentage of total image area occluded.
        \end{tablenotes}
    \end{threeparttable}
    \caption{\textbf{Spatial Robustness Limits for Images (Stable Diffusion 1.5).} Empirical breakdown points for PRC under spatial desynchronization.}
    \label{tab:image_spatial_limits}
\end{table}

\paragraph{Discussion}
Across cropping attacks, we observe a consistent two-stage failure mode in which decoding fails before detection, characteristic of deletion-channel behavior. Rotations cause immediate failure due to complete spatial misalignment. These image-based failure modes closely parallel the temporal desynchronization effects observed in the video domain, motivating our subsequent analysis of PRC under temporal attacks in Open Sora 1.3.

\subsection{Open Sora 1.3}
Following the above, we extend the PRC watermark to the video domain using Open Sora 1.3, a high-fidelity, open-source model for text-to-video generation. We approached this problem through three iterative implementations, moving from a naive theoretical extension to a more practical solution.

Crucially, Open Sora 1.3 operates on a compressed spatiotemporal latent space. The model compresses temporal data such that 1 latent frame corresponds to 4 real video frames. Consequently, our watermarking key $K$ exists in the dimensions $(W \times H \times T_{latent})$, while the attacks occur on the generated video in the pixel domain ($W_{pixel} \times H_{pixel} \times T_{real}$).

\subsubsection{Attempt 1: The Native Spatiotemporal Extension}
To start, we directly extended the PRC embedding scheme from the 2D spatial latent space ($W \times H$) used in image models to the full 3D spatiotemporal latent space ($W \times H \times T_{latent}$). To do this, we generate a unique watermarking key $K \in \mathbb{R}^{W \times H \times T_{latent}}$ that spans the entire volume of the generated video. 

\textbf{Imperceptibility Analysis.} Before evaluating robustness, however, we first established the imperceptibility of the watermark by evaluating it on VBench, a standard video generation benchmark that has been shown to align well with human preferences \citep{zheng2025vbench2}. This benchmark is made up of 2000 curated image prompts that are targeted towards 16 different metrics comprising a quality, semantic, and total score. For each prompt, we generate 5 watermarked and unwatermarked 2-second video clips, each at a resolution of 360p.

\begin{figure}[H]
    \centering
    \begin{minipage}[c]{0.58\linewidth} % 'c' centers vertically
        \centering
        \resizebox{\linewidth}{!}{ % scales table to minipage width
        \begin{tabular}{c|ccc}
            \hline
             Model & Total (\%) & Quality (\%) & Semantic (\%) \\
             \hline
             Open-Sora 1.3 & \textbf{78.43} & \textbf{81.57} & 65.89\\
             Open-Sora 1.3 w/ PRC & 78.32 & 81.40 & \textbf{66.04}  \\ 
        \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.38\linewidth} % 'c' centers vertically
        \centering
        \includegraphics[width=\linewidth]{images/vbench_metrics.png}
    \end{minipage}
    \caption{Weighted VBench summary statistics (left), Full VBench metric comparison with standard error bars (right).}
    \label{fig:vbench}
\end{figure}

The results in Figure \ref{fig:vbench} show that the PRC watermark did not noticeably impact the generative ability of the Open Sora model. We conclude that, similar to the results shown in the original PRC paper, the distributions of the watermarked and unwatermarked Open-Sora samples remain very similar. 

\textbf{Robustness Failure.} However, while the visual quality was maintained, the temporal robustness proved to be a critical failure point. Although this native extension effectively utilizes the video model's shift-window attention and 3D positional encoding, it is extremely brittle in practice. Because the watermark is embedded in the latent space, but attacks occur in the real frame space, even minor temporal disruptions are catastrophic. For example, dropping a single real frame shifts the entire sequence, misaligning the subsequent groups of 4 real frames with their corresponding latent key slice $K_t$. The detector, expecting a perfect 4:1 alignment, fails to correlate the shifted pixels with the correct latent watermark. 

\subsubsection{Attempt 2: Temporal Broadcasting}
To address this fragility, we adopted a "Temporal Broadcasting" strategy. Instead of a unique key for every latent frame, we generate a single spatial watermark $k \in \mathbb{R}^{W \times H}$ and repeat it $T_{latent}$ times, creating a spatiotemporally redundant signal where every latent frame carries an identical watermark.

Theoretically, this redundancy should render the watermark robust to any temporal cropping; since every frame carries the same watermark $k$, the detector should not require a specific temporal index to recover the signal. However, in practice, this approach fails significantly due to the decoupling of the attack domain (pixels) and the embedding domain (latents).

More specifically, the core issue lies in the 4:1 temporal compression ratio of the Open Sora 1.3 autoencoder. When the watermark is embedded, it is done into latent frames, each of which is a compressed representation of 4 specific physical frames (e.g., Latent 0 represents Frames 0-3). Temporal attacks, such as clipping or frame dropping, operate on the physical frames. If an attack removes a number of frames that is not a multiple of 4 (e.g., removing the first frame), the alignment of the pixel groupings shifts. For instance, if we remove the first physical frame, the sequence [Frame 1, 2, 3, 4] is encoded instead, producing a latent vector completely distinct from the original watermarked latent generated from [Frame 0, 1, 2, 3]. Consequently, naive padding (e.g., appending black frames to the end) fails to correct this alignment, rendering the watermark undetectable under front-clipping or random frame dropping attacks.

Additionally, we observed that subsampling attacks (e.g., retaining every 4th frame) lead to a near-total collapse of detection. This is expected behavior: since the Open Sora 1.3 encoder requires 4 consecutive physical frames to construct a single valid latent representation, feeding it disjoint frames (e.g., frames 0, 4, 8, 12) deprives the encoder of 75\% of the required temporal information. The resulting latent vectors are heavily distorted, destroying the watermark signal.

Interestingly, we found that the first latent frame remained detectable even under severe subsampling. This suggests that while temporal aliasing destroys the watermark signal in subsequent latents, the initial latent frame—acting as a sequence anchor—preserves enough spatial fidelity to trigger detection. This hypothesis is further supported by our finding that removing just the first physical frame (Frame 0) prior to subsampling eliminates this residual detection entirely. This indicates that the watermark's survival is not merely a property of the first latent block, but is strictly contingent on the presence of the very first physical frame, likely due to the autoencoder's initialization.

\subsubsection{Attempt 2.5: Modulo-4 Alignment Strategy}
The failure of Attempt 2 (summarized in Table \ref{tab:ablation_study}) clarified that simply repeating the key is insufficient; the phase of the real-to-latent compression also matters. Because Open Sora 1.3 encodes video with a ratio of 4 real frames to 1 latent frame, a temporal shift/crop that is not a multiple of 4 disrupts the latent representation of the watermark. 

To solve this, we implemented the \textbf{Modulo-4 Alignment Strategy}. Instead of a single padding configuration, our detector evaluates four distinct temporal alignments. We shift the incoming real video frames by an offset $i \in \{0, 1, 2, 3\}$ before encoding them back into the latent space (and padding the remainder). By cycling through these "groups of 4," we ensure that at least one configuration correctly aligns the real frames to the latent used during generation.

This refined strategy successfully mitigates the synchronization errors observed in previous attempts. As shown in Table \ref{tab:ablation_study}, Attempt 2.5 achieves broad robustness across all attack vectors where Attempt 2 failed. We further quantify the precise operating limits of our method in Table \ref{tab:stress_test}, demonstrating that the watermark remains reliably detectable even when over 55\% of the video content is removed or when frames are dropped with a probability of $p=0.75$.

\begin{table}[h]
    \centering
    \begin{threeparttable}
        \begin{tabular}{lcc}
            \toprule
            \textbf{Temporal Attack} & \textbf{Decodable Limit} & \textbf{Detectable Limit} \\
            \midrule
            \textbf{Boundary Clipping} & & \\
            \hspace{3mm} \% Video Removed & $< 35\%$ removed & $< 55\%$ removed \\
            \midrule
            \textbf{Frame Dropping} & & \\
            \hspace{3mm} Drop Probability ($p$) & $p < 0.75$ & $p < 0.75$ \\
            \midrule
            \textbf{Subsampling} & & \\
            \hspace{3mm} Max Factor ($k$) & Tested up to $5\times$\textsuperscript{*} & Tested up to $5\times$\textsuperscript{*} \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \small
            \item \textsuperscript{*} See Table \ref{tab:ablation_study} for architectural caveats regarding subsampling.
        \end{tablenotes}
    \end{threeparttable}
    \vspace{2mm}
    \caption{\textbf{Operating Limits of Attempt 2.5 (Modulo-4 Alignment).} We evaluate the breakdown point of our proposed method under severe temporal distortion. The watermark remains reliably detectable even when over half the video is removed.}
    \label{tab:stress_test}
\end{table}

\begin{table}[h]
    \centering
    \begin{threeparttable}
        % \resizebox{\linewidth}{!}{%
        \begin{tabular}{lcccc}
            \toprule
            & \multicolumn{2}{c}{\textbf{Attempt 1: Native 3D}} & \textbf{Attempt 2} & \textbf{Attempt 2.5 (Ours)} \\
            \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
            \textbf{Attack Type} & \textit{Pad End} & \textit{Pad Front} & \textit{Pad End} & \textbf{Modulo-4 Align} \\
            \midrule
            \textbf{Boundary Clipping} & & & & \\
            \hspace{3mm} Clip End (Back) & \cmark & \xmark & \cmark & \cmark \\
            \hspace{3mm} Clip Start (Front) & \xmark & \cmark & \textbf{$\sim$}\textsuperscript{\textdagger} & \cmark \\
            \midrule
            \textbf{Temporal Sampling} & & & & \\
            \hspace{3mm} Frame Dropping & \xmark & \xmark & \cmark & \cmark \\
            \hspace{3mm} Subsampling & \xmark & \xmark & \textbf{$\sim$}\textsuperscript{*} & \textbf{$\sim$}\textsuperscript{*} \\
            \bottomrule
        \end{tabular}
        % }
        \begin{tablenotes}
          \small
          \item \cmark = Robust \quad \xmark = Fail \quad \textbf{$\sim$} = Conditional Success
          \item \textsuperscript{\textdagger} \textbf{Phase Mismatch:} Only detectable if the remaining frame count is congruent to the original length modulo 4 (i.e., $L \equiv T \pmod 4$).
          \item \textsuperscript{*} \textbf{Anchor Effect:} Detection confined exclusively to the first latent frame
        \end{tablenotes}
        \caption{\textbf{Robustness Ablation Study.} Comparison of architectural failures. \textbf{Attempt 1} requires perfect index matching. \textbf{Attempt 2} suffers from phase alignment issues (front clipping). \textbf{Attempt 2.5} resolves the alignment issues, though subsampling remains limited to the first latent}
        \label{tab:ablation_study}
    \end{threeparttable}
\end{table}

% \begin{itemize}
%     \item Clipping video to be shorter (beginning, end, middle, random frames)
%     \item Speed Manipulation (Sub-sampling or double-sampling)
%     \item swapping frames
%     \item reversing the video
%     \item Normal cropping of the video
%     \item Looping???
% \end{itemize}
% Human Eval Distinguishability (given 2 videos, 1 watermarked, 1 unwatermarked) pick the watermarked image (run across 10 videos 4 ppl, and avg)

\section{Discussion}
\subsection{Failures of PRC}
While we have shown that the technique of PRC in images can extended to videos, we find that this natural extension is not sufficient for withstanding temporal attacks. Our results fail to achieve a desirable tradeoff between capacity and temporal robustness. In fact, we find cropping and rotations attacks in images to be comparable failure modes to the temporal attacks on videos. These failures can be explained by the VAE architectures used in both image and video models. Encoders are designed to compress the sample space into a smaller latent space. The crucial characteristic of these encoders is that they preserve local structure. For example, the 2D autoencoder used in Stable Diffusion preserves the structure of the spatial dimensions via a CNN architecture forming a patch code book \citep{esser2021tamingtransformershighresolutionimage, rombach2021highresolution}. The 3D autoencoder used in Open Sora builds on top of a 2D autoencoder by first compressing the spatial dimensions, and then using a causal temporal 3D CNN, compresses every 4 frames into a single latent \citep{yu2024languagemodelbeatsdiffusion, opensora}. In summary, the model architectures of image and video autoencoders are precisely designed to preserve the structure of their corresponding sample space's spatial and temporal dimensions. This structure preservation causes simple geometric and temporal attacks to destroy the PRC watermark. A rotation of an image or the removal of the first frames in a video completely offset the original latent structure, potentially causing a deletion attack on the watermark code. A deletion attack at the beginning of the code will make the entire code neither detectable nor decodable since it will cause the entire code to shift whereas the code is only robust to substitution \citep{gunn2025undetectable}.

\section{Future Directions}

\textbf{Exploiting the Anchor Frame Effect.} Our discovery that watermark detection under subsampling persists only in the first latent frame suggests that video VAEs prioritize sequence initialization. Future work should analyze the internal workings to determine if this bias can be systematically exploited. Schemes that explicitly target these initialization anchors—or artificially induce them periodically—could significantly enhance robustness against frame rate conversion.

\textbf{Generalization to Emerging Architectures.} The synchronization challenges identified in Open Sora 1.3 likely extend to any latent video model with temporal compression. It would be interesting to evaluate state-of-the-art architectures like Wan2.1, generalizing to new architecture changes induced in more recent works.

\textbf{Expanding Capacity via Temporal Modulation.} While "Temporal Broadcasting" maximizes robustness through redundancy, it limits capacity. We hypothesize that the temporal dimension offers unexploited bandwidth that could support significantly higher capacity without paying the tradeoffs found in the naive extension. Future research should investigate embedding schemes that decouple capacity from pure redundancy, aiming to improve information density while maintaining resilience against temporal attacks.

\section{Conclusion}
As generative video models increasingly improve, ensuring the provenance of synthetic media becomes a critical defense against misinformation. In this work, we extended the PRC watermark to the video domain, exposing a fundamental vulnerability in naive implementations: the synchronization mismatch between pixel-domain attacks and latent-domain compression. By introducing the Modulo-4 Alignment Strategy, we bridged this gap, achieving a watermark robust to severe temporal clipping and stochastic frame dropping.

\bibliographystyle{plainnat}  
\bibliography{references}

\end{document}


%%%%%%
%% Some comments about useful packages
%% (extract from bare_conf.tex by Michael Shell)
%%

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment for describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array

% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig


% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm's dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


% *** PDF and URL PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.



% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%%%%%%


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: