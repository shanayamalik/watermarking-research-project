%% LaTeX Template for ISIT 2025
%%
%% by Stefan M. Moser, October 2017
%% (with minor modifications by Tobias Koch, November 2023 and Michèle Wigger, November 2024)
%% 
%% derived from bare_conf.tex, V1.4a, 2014/09/17, by Michael Shell
%% for use with IEEEtran.cls version 1.8b or later
%%
%% Support sites for IEEEtran.cls:
%%
%% http://www.michaelshell.org/tex/ieeetran/
%% http://moser-isi.ethz.ch/manuals.html#eqlatex
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%%

%% 0. Abstract
%% 1. Introduction (detailed problem motivations)
%% 2. Classic Watermarking
%% 3. Stable Diffusion Watermarking (Tree-ring, PRC, T2S)
%% 4. SOTA Video Watermarking
%% 5. Evaluations
%% 6. Different Latent Position Watermarking

\documentclass[onecolumn]{IEEEtran}

%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}

%%%%%%
%% Packages:
%% Some useful packages (and compatibility issues with the IEEE format)
%% are pointed out at the very end of this template source file (they are 
%% taken verbatim out of bare_conf.tex by Michael Shell).
%
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{ifthen}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{mathrsfs} 
\usepackage{float}
\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
                             % with IEEE Xplore (see bare_conf.tex)

%% Please note that the amsthm package must not be loaded with
%% IEEEtran.cls because IEEEtran provides its own versions of
%% theorems. Also note that IEEEXplore does not accepts submissions
%% with hyperlinks, i.e., hyperref cannot be used.



\interdisplaylinepenalty=2500 % As explained in bare_conf.tex


% ------------------------------------------------------------
\begin{document}
\title{On Spatial-Temporal Robust Watermarking With Pseudo Random Codes}
% \title{Identifying AI Content With Imperceptible Watermarks}

% %%% Single author, or several authors with same affiliation:
% \author{%
%  \IEEEauthorblockN{Author 1 and Author 2}
% \IEEEauthorblockA{Department of Statistics and Data Science\\
%                    University 1\\
 %                   City 1\\
  %                  Email: author1@university1.edu}% }


%%% Several authors with up to three affiliations:
\author{%
  \IEEEauthorblockN{Shai Dickman},
  \IEEEauthorblockN{Shanaya Malik},
  \IEEEauthorblockN{Alex Tong},
  \IEEEauthorblockN{Esha Garg}
  \begin{center}
    \today
  \end{center}
}




\maketitle

%%% SHAI %%%
\begin{abstract}
The introduction of generative models has ushered in an era of eerily realistic synthetic content across a broad range of media, such as images, audio, and language. Although the capabilities of these models are certainly impressive, the rapid adaptation raise serious concerns about misinformation and copyright abuse. As a result, it is crucial to have reliable methods for identifying artificially generated content. Watermarking addresses this need by embedding a statistical signal during generation that indicates AI origin. Unlike classical watermarking, generative models allow such signals to be integrated directly into the sampling process, enabling stronger robustness and improved undetectability.
% Although the capabilities of these models are certainly impressive, it is important to reflect on their future implications. These models pose significant societal risks, such as enabling the spread of misinformation and making copyright enforcement more difficult. Thus, it is crucial to have reliable methods for identifying artificially generated content, such as watermarking. The goal of a watermark is to embed a strong statistical indicator that content originated from an AI system. This problem is distinct from classical watermarking because the generative processes inherent to AI models can be used to embed watermarks in less detectable and more robust ways. 
However, while watermarking has been studied extensively for language and image models, video generation models remain relatively underexplored despite their growing presence on social media platforms. The temporal aspect of videos introduces new vulnerabilities—such as clipping and frame dropping. In this report, we study the PRC watermark \citep{gunn2025undetectable} and examine its application to video models, specifically analyzing its robustness to temporal attacks and noting the trade-offs between capacity, imperceptibility, robustness, and undetectability.
\end{abstract}

\section{Background}
From an information-theoretic perspective, watermarking can be modeled as a communication problem over a noisy channel. We want to guarantee reliable communication of an embedded watermark across a family of adversarial channels, subject to imperceptibility constraints. More specifically, for robustness we seek to minimize the probability of decoding error $P_\epsilon$ over a family of channels $\{C_i\} \subseteq \mathcal{A}$, where $\mathcal{A}$ denotes the set of adversarial channels:
\[\sup_{C_i \in \mathcal{A}}P_\epsilon(C_i) < \delta\] for some small $\delta$. As we will discuss later, these adversarial channels can include attacks on a sample (image, video, etc.) as well as the watermark extraction process if such a process is imperfect.

In addition to robustness, watermarking must also satisfy imperceptibility, which can be modeled as a distortion constraint. Unlike classical channel coding, where the goal is to maximize rate, watermarking has an additional constraint: the distortion it introduces must fit within a perceptual budget (hidden from humans):
\begin{equation}
    \mathbb{E}[d(X,Y)] \leq D_{max}
\end{equation}
where $d(\cdot)$ is the distortion measure, $Y$ is the watermarked signal (image), and $X$ is the original. Overall, this makes watermarking a joint rate–distortion and channel coding problem. In practice, a watermarked image may look completely different from a non-watermarked image for the same random seed; however, what truly matters is that the distributions of $X$ and $Y$ are similar. Thus, the distortion measure $d$ is often a distance metric between distributions rather than between individual watermarked and non-watermarked images.


%%%%%% SHANAYA 
\subsection{Classic Watermarking Techniques}
Digital watermarking has evolved from simple spatial modifications of pixel values to frequency-domain embeddings that manipulate transform coefficients to achieve greater robustness against attacks, such as compression and cropping. 
\begin{itemize}
    \item \textbf{Least Significant Bit (LSB):} The LSB spatial domain technique embeds information by modifying the lowest bit planes of pixel values. It offers high capacity and imperceptibility, but our experiments confirmed its extreme fragility; it survives lossless operations but fails under virtually any form of compression or geometric transformation.
    \item \textbf{Discrete Cosine Transform (DCT):} Widely used in JPEG compression, DCT watermarking embeds signals into frequency coefficients. It provides a middle ground, offering robustness against compression (as the domain matches the compression algorithm) but often struggling with geometric attacks like cropping or rescaling without synchronization mechanisms.
    \item \textbf{Discrete Fourier Transform (DFT):} DFT techniques embed watermarks in the magnitude or phase of the frequency spectrum. Our evaluation highlights DFT as a particularly robust candidate, maintaining high detection confidence across rotation, scaling, and translation (RST) attacks due to the global nature of the transform.
    \item \textbf{Discrete Wavelet Transform (DWT):} Although not part of our primary experimental suite, DWT is a standard technique that decomposes images into multi-resolution sub-bands (approximation and detail coefficients). It allows for watermarking that closely models the Human Visual System (HVS), balancing robustness and imperceptibility.
    \item \textbf{Tree-Ring Watermarking:} Specific to diffusion models, this technique leverages the concentric frequency artifacts ("tree rings") naturally produced by the generative process. Our "Stable Diffusion-inspired" ring mask mimics this by injecting a circular band in the frequency domain, bridging the gap between classic frequency watermarks and generative artifacts.
\end{itemize}


%%%%%% SHANAYA
\subsection{Model-Specific vs. Ad-Hoc Watermarking}
The advent of high-fidelity diffusion models necessitates a paradigm shift from ad-hoc post-processing to model-specific watermarking.

\textbf{Ad-Hoc Watermarking:} Traditional methods (LSB, DCT, DFT) are applied \textit{post-hoc}, treating the generated image as a static asset. While universally applicable, they are often vulnerable to removal. For instance, a simple Gaussian blur or re-encoding can destroy an LSB watermark, and geometric shifts can desynchronize DCT blocks. Furthermore, they do not exploit the generative priors of the model.

\textbf{Model-Specific Watermarking:} Embedding watermarks directly into the generative process (e.g., within the latent space of a Stable Diffusion model) offers distinct advantages:
\begin{enumerate}
    \item \textbf{Deep Integration:} The watermark is "baked" into the image during the reverse diffusion process. This makes the signal an integral part of the image structure rather than a fragile overlay.
    \item \textbf{Rate-Distortion Optimization:} From an information-theoretic perspective, watermarking is a communication problem over a noisy channel. By embedding in the latent space, we can leverage the model's decoder to minimize perceptual distortion ($E[d(X,Y)] \le D_{max}$) while maximizing the reliability of the signal against the "channel noise" of the diffusion process itself.
    \item \textbf{Robustness to Generative Attacks:} Watermarks native to the model's latent space are better positioned to survive "purification" attacks (e.g., adding noise and re-generating), as the watermark is consistent with the model's internal representation.
\end{enumerate}
Our research investigates this trade-off, comparing how classic image-space injections (DCT, DFT) fare against latent-inspired structures (Ring Mask) under standard perturbations.

%%% SHAI %%%
\section{Generative Model Inference$\And$Inversion}
As described earlier, it can be valuable to embed watermarks within the sampling process of a generative model rather than after generation. Most image and video generation models use latent space iterative processes that are initialized with Gaussian noise. A common technique is to watermark this initial Gaussian latent through some detectable transformation. To  extract the watermark, however, there needs to be a way to predict the initial latent from a given sample. Thus, an understanding of the inner workings of these generative models is important. We provide a brief overview of the sampling and inverse sampling methods of Stable Diffusion 1.5 \citep{rombach2021highresolution}, a Diffusion image model, and Open Sora 1.3 \citep{opensora}, a Flow-Matching video model. In the following sections, we investigate how to watermark outputs from these models by using corresponding sampling and inverse sampling algorithms.

\subsection{Stable Diffusion 1.5}
\begin{figure}                 
    \centering
    \includegraphics[width=0.48\linewidth]{images/stablediffusiongen.png}
      \quad\quad
    \includegraphics[width=0.38\linewidth]{images/intermediates.png}
    \caption{Stable Diffusion model architecture \citep{rombach2021highresolution} (left), Stable Diffusion 1.5 example: "Claude Shannon holding a red balloon" (right).}
\end{figure}

The iterative processes that define Diffusion models are called the forward and reverses processes. Figure 1 shows these processes in the Stable Diffusion architecture. Forward diffusion iteratively adds Gaussian noise to a data point $x_0$ to achieve a final noised result $x_T$. Here, $x_0$ is the latent representation of an image, which is derived by a model encoder. There is also a corresponding decoder which converts latents to images for the reverse process. The forward process can be written in a closed form as
\begin{equation}
    x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon, \quad \epsilon\sim\mathcal{N}(0,I)
\end{equation}
where $\bar{\alpha}_t$ is a function of the scheduled variances for the iteratively added noise and the $\epsilon$ term represents the cumulative Gaussian noise. Inference is done via the reverse (or denoising) process which generates a sample latent given an initial Gaussian latent. An example of this process is depicted in Figure 1. Deep Diffusion Implicit Model (DDIM) is a commonly used process for sampling and is used to sampling from Stable Diffusion 1.5 \citep{song2022denoisingdiffusionimplicitmodels, rombach2021highresolution}. At each denoising step, we form a prediction $\epsilon_\theta$ of the cumulative noise added to $x_0$ to get $x_t$ during the forward process. We use this noise prediction to form an estimate $\hat{x}_0^t$ of the initial sample $x_0$. Finally, we predict $x_{t-1}$ using the DDIM update step:
\begin{equation}
    x_{t-1}=\sqrt{\bar{\alpha}_{t-1}}\hat{x}_0^t + \sqrt{1-\bar{\alpha}_{t-1}}\epsilon_\theta(x_t)
\end{equation}


We approximate the inversion of this method in the same way as the Tree-ring watermarking paper as well as several others \citep{wen2023treeringwatermarksfingerprintsdiffusion}. This inversion assumes $x_t-x_{t-1}\approx x_{t+1}-x_t$. It is as follows:
\begin{equation}
    x_{t+1}=\sqrt{\bar{\alpha}_{t+1}}\hat{x}_0^t + \sqrt{1-\bar{\alpha}_{t+1}}\epsilon_\theta(x_t)
\end{equation}
This method is an approximation and will accumulate errors from each additional step.

\subsection{Open Sora 1.3}
Flow-Matching is another generative process with strong parallels to Diffusion model, however formulated in a distinct way. The forward process is a linear deterministic interpolation between the data point $x_0$ and a noise term $\epsilon$ \citep{lipman2023flowmatchinggenerativemodeling}. In Open Sora, $x_0$ is the latent representation of an entire video. Like Stable Diffusion, Open Sora uses an encoder and decoder to map between pixel/temporal space and latent space. It also lets the noise term $\epsilon$ be Gaussian which makes the forward process correspond to a Diffusion forward process. The distinct difference between the latent representation of Open Sora and Stable Diffusion is that Open Sora uses an additional temporal dimension. Also, while Diffusion models are trained to predict the additive noise at a given timestep, Flow-Matching models are trained as vector fields $v_\theta$ that can be integrated via standard ODE solvers to derive samples $\hat{x}_0$ from an initial state of Gaussian noise \citep{lipman2023flowmatchinggenerativemodeling}. These two methods are very similar and in fact the DDIM update step can be shown to have the same structure as Euler's method, a first-order accurate scheme for solving ODEs and a popular method used in Flow-Matching inference \citep{xu2025unveilinversioninvarianceflow}. The Flow-Matching update step is:
\begin{equation}
    x_{n-1}=x_n+(t_n-t_{n-1})v_\theta(x_n;t_n)
\end{equation}


Operating under the same assumption as DDIM inversion that $x_t-x_{t-1}\approx x_{t+1}-x_t$, we use the following inverse step:
\begin{equation}
    x_n=x_{n-1}-(t_n-t_{n-1})v_\theta(x_{n-1};t_n)
\end{equation}


Like DDIM, there are accuracy concerns with this inversion since it is still an approximation. For an exact inversion, we can solve the implicit update step
\begin{equation}
    x_n=x_{n-1}-(t_n-t_{n-1})v_\theta(x_n;t_n)
\end{equation}
but this is computationally more demanding as it will require multiple fixed point iterations. This method has been used for image editing which requires higher accuracy inversion \citep{xu2025unveilinversioninvarianceflow}, however, like with DDIM, we found that an approximate inversion is sufficient for extracting the watermark.

\subsection{Consequences of Approximate Inversions}
It is important to acknowledge an additional challenge of injecting a watermark into the initial latent of a generative model: the inverse process to predict the initial latent can in itself degrade the watermark, even without any attacks on the original sample. The inverse processes for Stable Diffusion and Open Sora, while slightly different, are both approximations and accumulate error. In fact, both inversions are first-order accurate, which means that the error introduced is $O(1/N)$ where $N$ is the number of Diffusion or Flow-Matching iterations.

Let $X$ be the watermarked image, $Y$ be the attacked image, and $f(Y)$ be the predicted noise latent from the attacked image. We can view the errors introduced by $f$ as just additional image attacks. Then $X-Y-f(Y)$ form a Markov Chain. By the Data Processing Inequality, $I(X,f(Y))\leq I(X, Y)$. It follows that the capacity of a watermark extracted from the approximate inverted latent representation $f(Y)$ is upper bounded by the capacity of a watermark extracted directly from $Y$. Thus, if $f$ is not a bijection, we will theoretically have lower watermark capacity by embedding it into the initial Gaussian latent. However, we argue that the benefits in imperceptibility and the unlearnability of the watermark outweigh these faults.

\input{prc-overview}

\section{Experiments}
\subsection{Experiment Setup}
While the crux of our discussion revolves around video watermarking, it is important to first draw an analogy to watermarking for image generation. As a result, we start by studying the effects of cropping attacks on watermarked images. We note that this specific attack is not mentioned \citep{gunn2025undetectable} nor in many previous watermarking papers. We hypothesize that this is because cropping is a destructive attack, destroying image content as well as the watermark. Furthermore, aggressive cropping reduces image utility.

In contrast, desynchronization attacks are a very common concern in video watermarking. Temporal operations such as clipping, frame interpolation, and frame-dropping preserve most of the semantic content and practical utility of a video, and frequently arise both from human-induced post-processing and standard transmission pipelines (e.g., packet loss). As a result, temporal misalignment is not a purely adversarial setting but an inherent property of real-world video distribution. 

As a result, we first study the effects of clipping on image generation as a precursor to video generation, then spend most of our exposition on temporal robustness against attacks like clipping, subsampling, frame interpolation, and frame dropping. 

\subsection{Stable 
Diffusion 1.5}
\begin{itemize}
    \item Resizing image and black out sections, black out random pixels (different types of cropping i.e. cropping borders/corners/middle of image, random sections of the image)
\end{itemize}

\subsection{Open Sora 1.3}
\begin{itemize}
    \item Clipping video to be shorter (beginning, end, middle, random frames)
    \item Speed Manipulation (Sub-sampling or double-sampling)
    \item swapping frames
    \item reversing the video
    \item Normal cropping of the video
    \item Looping???
\end{itemize}
Human Eval Distinguishability (given 2 videos, 1 watermarked, 1 unwatermarked) pick the watermarked image (run across 10 videos 4 ppl, and avg)

We evaluated the PRC watermark on VBench, a standard video generation benchmark that has been shown to align well with human preferences \citep{zheng2025vbench2}. This benchmark is made up of 2000 curated image prompts that are targeted towards 16 different metrics comprising a quality, semantic, and total score. For each prompt, we generate 5 watermarked and unwatermarked 2-second video clips, each at a resolution of 360p.

\begin{figure}[H]
    \centering
    \begin{minipage}[c]{0.58\linewidth} % 'c' centers vertically
        \centering
        \resizebox{\linewidth}{!}{ % scales table to minipage width
        \begin{tabular}{c|ccc}
            \hline
             Model & Total (\%) & Quality (\%) & Semantic (\%) \\
             \hline
             Open-Sora 1.3 & \textbf{78.43} & \textbf{81.57} & 65.89\\
             Open-Sora 1.3 w/ PRC & 78.32 & 81.40 & \textbf{66.04}  \\ 
        \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.38\linewidth} % 'c' centers vertically
        \centering
        \includegraphics[width=\linewidth]{images/vbench_metrics.png}
    \end{minipage}
    \caption{Weighted VBench summary statistics (left), Full VBench metric comparison with standard error bars (right).}
    \label{fig:vbench}
\end{figure}

The results in Figure \ref{fig:vbench} show that the PRC watermark did not noticeably impact the generative ability of the Open Sora model, thus establishing its strength in imperceptibility. This metric indicates that the distributions of the watermarked and unwatermarked Open-Sora samples are very similar.

\section{Discussion}
\subsection{Failures of PRC}
While we have shown that the technique of PRC in images can extended to videos, we find that this natural extension is not sufficient for withstanding temporal attacks. Our results fail to achieve a desirable tradeoff between capacity and temporal robustness. In fact, we find cropping and rotations attacks in images to be comparable failure modes to the temporal attacks on videos. These failures can be explained by the VAE architectures used in both image and video models. Encoders are designed to compress the sample space into a smaller latent space. The crucial characteristic of these encoders is that they preserve local structure. For example, the 2D autoencoder used in Stable Diffusion preserves the structure of the spatial dimensions via a CNN architecture forming a patch code book \citep{esser2021tamingtransformershighresolutionimage, rombach2021highresolution}. The 3D autoencoder used in Open Sora builds on top of a 2D autoencoder by first compressing the spatial dimensions, and then using a causal temporal 3D CNN, compresses every 4 frames into a single latent \citep{yu2024languagemodelbeatsdiffusion, opensora}. In summary, the model architectures of image and video autoencoders are precisely designed to preserve the structure of their corresponding sample space's spatial and temporal dimensions. This structure preservation causes simple geometric and temporal attacks to destroy the PRC watermark. A rotation of an image or the removal of the first frames in a video completely offset the original latent structure, potentially causing a deletion attack on the watermark code. A deletion attack at the beginning of the code will make the entire code neither detectable nor decodable since it will cause the entire code to shift whereas the code is only robust to substitution \citep{gunn2025undetectable}.
\section{Future Directions}
Apply to Wan2.1 (see above discussion - probably should reorganize and bring into this section)

We predict that a higher message capacity can be achieved by using the PRC watermark across the temporal dimension, however establishing this empirically is left for future work.
\section{Conclusion}

\bibliographystyle{plainnat}  
\bibliography{references}

\end{document}


%%%%%%
%% Some comments about useful packages
%% (extract from bare_conf.tex by Michael Shell)
%%

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment for describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array

% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig


% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm's dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


% *** PDF and URL PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.



% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%%%%%%


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: