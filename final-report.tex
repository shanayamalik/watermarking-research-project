%% LaTeX Template for ISIT 2025
%%
%% by Stefan M. Moser, October 2017
%% (with minor modifications by Tobias Koch, November 2023, and Michèle Wigger, November 2024)
%% 
%% derived from bare_conf.tex, V1.4a, 2014/09/17, by Michael Shell
%% for use with IEEEtran.cls version 1.8b or later
%%
%% Support sites for IEEEtran.cls:
%%
%% http://www.michaelshell.org/tex/ieeetran/
%% http://moser-isi.ethz.ch/manuals.html#eqlatex
%% http://www.ctan.org/tex-archive/macros/latex/contrib/IEEEtran/
%%

%% 0. Abstract
%% 1. Introduction (detailed problem motivations)
%% 2. Classic Watermarking
%% 3. Stable Diffusion Watermarking (Tree-ring, PRC, T2S)
%% 4. SOTA Video Watermarking
%% 5. Evaluations
%% 6. Different Latent Position Watermarking

\documentclass[onecolumn]{IEEEtran}

%% depending on your installation, you may wish to adjust the top margin:
\addtolength{\topmargin}{9mm}

%%%%%%
%% Packages:
%% Some useful packages (and compatibility issues with the IEEE format)
%% are pointed out at the very end of this template source file (they are 
%% taken verbatim out of bare_conf.tex by Michael Shell).
%
% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%
\usepackage[utf8]{inputenc} 
\usepackage[T1]{fontenc}
\usepackage{url}
\usepackage{ifthen}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{amssymb}
\usepackage{mathrsfs} 
\usepackage{float}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\usepackage[cmex10]{amsmath} % Use the [cmex10] option to ensure complicance
                             % with IEEE Xplore (see bare_conf.tex)

%% Please note that the amsthm package must not be loaded with
%% IEEEtran.cls because IEEEtran provides its own versions of
%% theorems. Also note that IEEEXplore does not accept submissions
%% with hyperlinks, i.e., hyperref cannot be used.

\newtheorem{definition}{Definition}

\interdisplaylinepenalty=2500 % As explained in bare_conf.tex


% ------------------------------------------------------------
\begin{document}
\title{On Spatial-Temporal Robust Watermarking With Pseudo Random Codes}
% \title{Identifying AI Content With Imperceptible Watermarks}

% %%% Single author, or several authors with the same affiliation:
% \author{%
%  \IEEEauthorblockN{Author 1 and Author 2}
% \IEEEauthorblockA{Department of Statistics and Data Science\\
%                    University 1\\
 %                   City 1\\
  %                  Email: author1@university1.edu}% }


%%% Several authors with up to three affiliations:
\author{%
  \IEEEauthorblockN{Shai Dickman},
  \IEEEauthorblockN{Shanaya Malik},
  \IEEEauthorblockN{Alex Tong},
  \IEEEauthorblockN{Esha Garg}
  \begin{center}
   December 12, 2025
\end{center}
}




\maketitle

%%% SHAI %%%
\begin{abstract}
The introduction of generative models has ushered in an era of eerily realistic synthetic content across a broad range of media, including images, audio, and language. Although the capabilities of these models are undoubtedly impressive, the rapid adaptation raises serious concerns about misinformation and copyright abuse. As a result, it is crucial to have reliable methods for identifying artificially generated content. Generative watermarking addresses this challenge by embedding a statistical signal directly into the model's sampling process, offering superior robustness and undetectability compared to classical post-hoc methods. 
% Watermarking addresses this need by embedding a statistical signal during generation that indicates AI origin. Unlike classical watermarking, which applies it post hoc, generative models allow such signals to be integrated directly into the sampling process, enabling stronger robustness and improved undetectability.
% Although the capabilities of these models are certainly impressive, it is essential to reflect on their future implications. These models pose significant societal risks, such as enabling the spread of misinformation and making copyright enforcement more difficult. Thus, it is crucial to have reliable methods for identifying artificially generated content, such as watermarking. The goal of a watermark is to embed a strong statistical indicator that the content originated from an AI system. This problem differs from classical watermarking because the generative processes inherent to AI models can be used to embed watermarks in less detectable, more robust ways. 
However, while watermarking has been extensively studied for language and image models, video generation models remain relatively underexplored, despite their growing presence on social media platforms. The temporal aspect of videos introduces new vulnerabilities—such as clipping and frame dropping—that complicate detection. In this report, we extend the PRC watermark \citep{gunn2025undetectable} to the video domain, specifically analyzing its robustness against temporal attacks and characterizing the fundamental trade-offs between capacity, imperceptibility, robustness, and undetectability.
\end{abstract}

\section{Introduction}
% The rapid proliferation of generative artificial intelligence has fundamentally transformed the digital media landscape. While initial breakthroughs focused on text and images, recent advancements have enabled the creation of high-fidelity synthetic video content that is increasingly becoming indistinguishable from real videos. Proprietary and open-sourced models alike have demonstrated the ability to generate complex scenes with eerily realistic temporal consistency and camera motion, which will only improve over time. However, this democratized capability comes with significant societal risks. The ease of generating hyper-realistic deepfakes threatens to accelerate the spread of misinformation and complicate copyright enforcement. Consequently, establishing reliable provenance has become an urgent technical priority. 

To address the risks of AI-generated content, a new paradigm of watermarking has emerged: generative watermarking. Instead of embedding watermarks post-hoc, generative watermarking embeds a statistical signal directly into the video creation process, treating it as a communication channel, yielding watermarks that are inherently more robust and imperceptible than classical schemes. However, while watermarking static images has been well studied over the past few years, with relatively robust solutions against various spatial attacks, generative video watermarking remains significantly underexplored due to its novelty. Videos possess a temporal dimension, introducing a new class of adversarial attacks. In real-world pipelines, videos are frequently subjected to \textit{temporal desynchronization}: they may be clipped to shorter durations, dropped frames during streaming (packet loss), or subsampled to lower frame rates. For a watermarking system to be practical, it must be robust to these temporal distortions without requiring the detector to have access to meta-information such as which frames were removed. In this work, we address the challenge of robust video watermarking by extending the PRC watermark \citep{gunn2025undetectable} to the spatio-temporal domain. Using the Open Sora 1.3 architecture as our testbed, we demonstrate that naive extensions of image-based watermarking schemes fail catastrophically under temporal attacks and that a more careful construction is needed to achieve temporal robustness.

Our contributions are as follows:
\begin{itemize}
\item \textbf{Spatial Robustness Baseline}: We extend the evaluation of the PRC watermark for images under geometric attacks not covered in the original work, establishing a critical baseline for understanding the synchronization sensitivity that subsequently guides our video analysis.
\item \textbf{Evaluation of Natural Spatio-temporal Extension}: We formulate a natural extension of the PRC watermark from 2D spatial latents to 3D spatio-temporal latents. While we validate its imperceptibility using the VBench benchmark, we demonstrate that this naive approach is extremely brittle, failing even under minor temporal shifts due to the strict index-dependence of the 3D key.
\item \textbf{Analysis of Temporal Broadcasting \& Phase Mismatch}: To address this brittleness, we investigate "Temporal Broadcasting" (redundancy over time). We identify a critical "Phase Mismatch" vulnerability that causes this strategy to still fail under front-clipping attacks. We show this occurs because removing frames shifts the alignment of physical pixels relative to the model's fixed latent compression blocks (typically 4:1), scrambling the embedded signal.
\item \textbf{Modulo-4 Alignment Strategy}: We propose a robust detection protocol that resolves these synchronization issues by cycling through temporal phase shifts during extraction. We show that this strategy recovers the watermark under severe attacks, maintaining detectability even with $55\%$ video removal or random frame dropping with probability up to $p=0.75$.
\item \textbf{Discovery of the "Anchor Effect"}: We characterize the fundamental operating limits of our method, specifically identifying the "Anchor Effect" under subsampling attacks. We demonstrate that detection in subsampled videos is largely an artifact of the model's initialization, relying strictly on the presence of the very first physical frame to preserve the watermark signal.
\end{itemize}

\section{Background}
% \subsection{Problem Motivations}
% From an information-theoretic perspective, watermarking can be modeled as a communication problem over a noisy channel. We want to guarantee reliable communication of an embedded watermark across a family of adversarial channels, subject to imperceptibility constraints. More specifically, for robustness we seek to minimize the probability of decoding error $P_\epsilon$ over a family of channels $\{C_i\} \subseteq \mathcal{A}$, where $\mathcal{A}$ denotes the set of adversarial channels:
% \[\sup_{C_i \in \mathcal{A}}P_\epsilon(C_i) < \delta\] for some small $\delta$. As we will discuss later, these adversarial channels can include attacks on a sample (image, video, etc.) as well as the watermark extraction process if such a process is imperfect.

% In addition to robustness, watermarking must also satisfy imperceptibility, which can be modeled as a distortion constraint. Unlike classical channel coding, where the goal is to maximize rate, watermarking has an additional constraint: the distortion it introduces must fit within a perceptual budget (hidden from humans):
% \begin{equation}
%     \mathbb{E}[d(X,Y)] \leq D_{max}
% \end{equation}
% where $d(\cdot)$ is the distortion measure, $Y$ is the watermarked signal (image), and $X$ is the original. Overall, this makes watermarking a joint rate–distortion and channel coding problem. In practice, a watermarked image may look completely different from a non-watermarked image for the same random seed; however, what truly matters is that the distributions of $X$ and $Y$ are similar. Thus, the distortion measure $d$ is often a distance metric between distributions rather than between individual watermarked and non-watermarked images.

% \subsection{Background Watermarking Techniques}
Watermarking can be viewed as a communication problem: an encoder embeds a signal into content, an adversary applies a channel transformation (e.g., compression, cropping, re-encoding), and a decoder attempts detection or message recovery. A useful way to organize prior work is therefore not only by algorithmic construction, but by \emph{where} the watermark is embedded (pixel space, transform space, or model latent space) and \emph{which distortions} it is designed to survive.

% \paragraph{Why include classical baselines?}
% Classical post-hoc watermarking serves as an important reference point for this work. First, it represents the standard setting where a watermark is added \emph{after} content generation, without access to the generative process itself. Second, it exposes a recurring vulnerability that becomes even more pronounced in video: \emph{desynchronization}. Attacks that delete, shift, rotate, or resample content often break coordinate alignment, which is frequently more damaging than additive noise and motivates model-native alternatives.

\paragraph{Post-hoc watermarking (pixel and transform domains).}
We consider representative classical watermarking techniques that operate directly on generated samples:
\begin{itemize}
    \item \textbf{Least Significant Bit (LSB).}
    LSB embeds information by modifying the lowest bit planes of pixel values. While it offers high capacity and can be imperceptible under ideal conditions, it is extremely brittle in practice. In our experiments, LSB survived essentially only lossless operations and failed under typical compression, filtering, and spatial perturbations, confirming its unsuitability for adversarial settings.
    
    \item \textbf{Discrete Fourier Transform (DFT).}
    DFT-based watermarking embeds information in the magnitude or phase of the frequency spectrum. Because the Fourier representation captures more global image structure than raw pixels, DFT exhibits improved robustness to certain post-processing operations and mild geometric transformations. Among traditional methods, DFT was the strongest in our evaluation, outperforming LSB under pixel-level distortions and mild geometric transformations such as translation and rotation; however, it still fails under stronger geometric attacks like cropping and rescaling.
\end{itemize}

\paragraph{Generative watermarking (model-native embedding).}
Recent work moves watermarking upstream into the sampling process of diffusion and flow-matching models by modifying the initial Gaussian latent. 
\begin{itemize}
    \item \textbf{Tree-Ring Watermarking.}
    Tree-ring watermarking injects a circular band into the frequency domain of the initial latent, typically concentrating energy in higher frequencies to preserve perceptual quality. It demonstrates strong empirical robustness in image diffusion models \citep{wen2023treeringwatermarksfingerprintsdiffusion}.
    
    \item \textbf{Gaussian-Shading Watermarking.}
    Gaussian-shading biases the initial Gaussian latent toward a key-dependent region while preserving the marginal distribution of individual samples \citep{yang2024gaussianshadingprovableperformancelossless}. Despite its robustness, prior work reports a reduction in sample diversity, highlighting a trade-off between detectability and generative richness \citep{gunn2025undetectable}.
    
    \item \textbf{Pseudorandom Codes (PRC).}
    PRC embeds a pseudorandom code into the initial latent to achieve strong detectability without sacrificing sample diversity \citep{gunn2025undetectable}. A recent extension applies PRC to video by distributing the code across spatiotemporal latents and then recovering code alignments with edit distance calculations \citep{hu2025videomarkdistortionfreerobustwatermarking}, but evaluates only mild temporal distortions (e.g., single-frame drops). In this work, we study PRC under \emph{severe} spatial and temporal attacks and analyze how latent compression architectures fundamentally constrain robustness.
\end{itemize}

\paragraph{Summary and motivation.}
Classical post-hoc schemes degrade rapidly once the adversarial channel includes strong desynchronization, while generative watermarking gains leverage by embedding signals at the model’s natural initialization point. This distinction motivates our focus on PRC, which we evaluate across aggressive spatial and temporal attacks to characterize its operating limits and recovery strategies in both image and video generation models.

% \begin{table}[h]
%     \centering
%     \begin{threeparttable}
%         \begin{tabular}{lccc}
%             \toprule
%             \textbf{Method} & \textbf{Embedding Domain} & \textbf{Strength} & \textbf{Primary Failure Mode} \\
%             \midrule
%             LSB & Pixel space & High capacity, simple & Compression / mild filtering \\
%             DFT & Frequency space & Better post-processing  robustness & Severe desynchronization \\
%             Tree-Ring & Gen.\ latent (freq.) & Strong image robustness & Model / attack specificity \\
%             Gaussian-Shading & Gen.\ latent (distribution bias) & Robust initialization & Diversity degradation \\
%             PRC & Gen.\ latent (pseudorandom) & Fidelity + detectability & Spatial / temporal misalignment \\
%             \bottomrule
%         \end{tabular}
%     \end{threeparttable}

%     \vspace{2mm}
%     \caption{\textbf{Watermarking baselines and typical failure modes.}
%     The key distinction is post-hoc (pixel or transform domain) versus model-native (latent) embedding.}
%     \label{tab:wm_baselines}
% \end{table}

% Traditional ad-hoc methods such as LSB and DFT treat a sample image as a static asset. While universally applicable, they are often vulnerable to removal. For instance, a simple Gaussian blur or re-encoding can destroy an LSB watermark. Furthermore, they do not exploit the generative priors of the model. Embedding watermarks directly into the generative process (e.g., within the latent space of a Stable Diffusion model) offers distinct advantages:
% \begin{enumerate}
%     \item \textbf{Deep Integration:} The watermark is "baked" into the image during the reverse diffusion process. This makes the signal an integral part of the image structure rather than a fragile overlay.
%     \item \textbf{Rate-Distortion Optimization:} From an information-theoretic perspective, watermarking is a communication problem over a noisy channel. By embedding in the latent space, we can leverage the model's decoder to minimize perceptual distortion ($E[d(X,Y)] \le D_{max}$) while maximizing the reliability of the signal against the "channel noise" of the diffusion process itself.
%     \item \textbf{Robustness to Generative Attacks:} Watermarks native to the model's latent space are better positioned to survive "purification" attacks (e.g., adding noise and re-generating), as the watermark is consistent with the model's internal representation.
% \end{enumerate}
% Our research investigates the PRC watermark which can be embedded in the generative process. We evaluate how this watermark has been used for images, how it can be extended to videos, and how robust it is to spatial and temporal attacks.

\section{Generative Model Inference$\And$Inversion}
As described earlier, it can be valuable to embed watermarks within the sampling process of a generative model rather than after generation. Most image and video generation models use latent space iterative processes that are initialized with Gaussian noise. A common technique is to watermark this initial Gaussian latent through some detectable transformation. To extract the watermark, there needs to be a way to predict the initial latent from a given sample. Thus, an understanding of the inner workings of these generative models is important. We provide a brief overview of the sampling and inverse sampling methods of Stable Diffusion 1.5 \citep{rombach2021highresolution}, a Diffusion image model, and Open Sora 1.3 \citep{opensora}, a Flow-Matching video model. In the following sections, we investigate how to watermark outputs from these models by using corresponding sampling and inverse sampling algorithms.

\subsection{Pretrained Variational Autoencoders}
A Variational Autoencoder (VAE) is a model that encodes a sample into a latent representation and then decodes the latent closely to its original state. VAE models contain an encoder and decoder and pretrained VAEs are often used in generative models that act on a latent space. The encoders are explicitly designed to compress the sample space while preserving local structure. Stable Diffusion’s 2D autoencoder maintains spatial fidelity via a patch-based code-book \citep{esser2021tamingtransformershighresolutionimage, rombach2021highresolution}. Open Sora’s 3D autoencoder uses similar 2D compression as well as causal 3D CNNs to compress groups of four frames into a single latent \citep{yu2024languagemodelbeatsdiffusion, opensora}. The structure preservation of encoders is relevant to understand how to inject a watermark in a latent since the structure of the injection will likely be preserved in the corresponding decoded sample.

\subsection{Stable Diffusion 1.5 Sampling}
\begin{figure}                 
    \centering
    \includegraphics[width=0.48\linewidth]{images/stablediffusiongen.png}
      \quad\quad
    \includegraphics[width=0.38\linewidth]{images/intermediates.png}
    \caption{Stable Diffusion model architecture \citep{rombach2021highresolution} (left), Stable Diffusion 1.5 example: "Claude Shannon holding a red balloon" (right).}
\end{figure}

The iterative processes that define Diffusion models are called the forward and reverse processes. Figure 1 shows these processes in the Stable Diffusion architecture. Forward diffusion iteratively adds Gaussian noise to a data point $x_0$ to achieve a final noised result $x_T$. Here, $x_0$ is the latent representation of an image, which is derived by a pretrained encoder. There is also a corresponding decoder that converts latents to images for the reverse process. The forward process can be written in a closed form as
\begin{equation}
    x_t=\sqrt{\bar{\alpha}_t}x_0+\sqrt{1-\bar{\alpha}_t}\epsilon, \quad \epsilon\sim\mathcal{N}(0,I)
\end{equation}
where $\bar{\alpha}_t$ is a function of the scheduled variances for the iteratively added noise and the $\epsilon$ term represents the cumulative Gaussian noise. Inference is done via the reverse (or denoising) process which generates a sample latent given an initial Gaussian latent. An example of this process conditioned on a test prompt is depicted in Figure 1. Denoising Diffusion Implicit Models (DDIM) is a commonly used process for sampling and is used to sample from Stable Diffusion 1.5 \citep{song2022denoisingdiffusionimplicitmodels, rombach2021highresolution}. At each denoising step, we form a prediction $\epsilon_\theta$ of the cumulative noise added to $x_0$ to get $x_t$ during the forward process. We use this noise prediction to form an estimate $\hat{x}_0^t$ of the initial sample $x_0$. Finally, we predict $x_{t-1}$ using the DDIM update step:
\begin{equation}
    x_{t-1}=\sqrt{\bar{\alpha}_{t-1}}\hat{x}_0^t + \sqrt{1-\bar{\alpha}_{t-1}}\epsilon_\theta(x_t)
\end{equation}


We approximate the inversion of this method in the same way as the Tree-ring watermarking paper as well as several others \citep{wen2023treeringwatermarksfingerprintsdiffusion}. This inversion assumes $x_t-x_{t-1}\approx x_{t+1}-x_t$. It is as follows:
\begin{equation}
    x_{t+1}=\sqrt{\bar{\alpha}_{t+1}}\hat{x}_0^t + \sqrt{1-\bar{\alpha}_{t+1}}\epsilon_\theta(x_t)
\end{equation}
This method is an approximation and will accumulate errors from each additional step.

\subsection{Open Sora 1.3 Sampling}
Flow-Matching is another generative process with strong parallels to Diffusion models; however, it is formulated differently. The forward process is a linear deterministic interpolation between the data point $x_0$ and a noise term $\epsilon$ \citep{lipman2023flowmatchinggenerativemodeling}. In Open Sora, $x_0$ is the latent representation of an entire video. Like Stable Diffusion, Open Sora uses an encoder and decoder to map between pixel/temporal space and latent space. It also lets the noise term $\epsilon$ be Gaussian, which makes the forward process correspond to a Diffusion forward process. The key difference between the latent representation of Open Sora and Stable Diffusion is that Open Sora utilizes an additional temporal dimension. In addition, while Diffusion models are trained to predict the additive noise at a given timestep, Flow-Matching models are trained as vector fields $v_\theta$ that can be integrated via standard ODE solvers to derive samples $\hat{x}_0$ from an initial state of Gaussian noise \citep{lipman2023flowmatchinggenerativemodeling}. These two methods are very similar; in fact, the DDIM update step shares the same structure as Euler's method, a first-order accurate scheme for solving ODEs commonly used in Flow-Matching inference \citep{xu2025unveilinversioninvarianceflow}. The Flow-Matching update step is:
\begin{equation}
    x_{n-1}=x_n+(t_n-t_{n-1})v_\theta(x_n;t_n)
\end{equation}


Operating under the same assumption as DDIM inversion that $x_t-x_{t-1}\approx x_{t+1}-x_t$, we use the following inverse step:
\begin{equation}
    x_n=x_{n-1}-(t_n-t_{n-1})v_\theta(x_{n-1};t_n)
\end{equation}


Like DDIM, there are accuracy concerns with this inversion since it is still an approximation. For an exact inversion, we can solve the implicit update step
\begin{equation}
    x_n=x_{n-1}-(t_n-t_{n-1})v_\theta(x_n;t_n)
\end{equation}
but this is computationally more demanding as it will require multiple fixed-point iterations. This method has been used for image editing which requires higher accuracy inversion \citep{xu2025unveilinversioninvarianceflow}, however, like with DDIM, we found that an approximate inversion is sufficient for extracting the watermark.

\subsection{Consequences of Approximate Inversions}
It is important to acknowledge an additional challenge of injecting a watermark into the initial latent of a generative model: the inverse process to predict the initial latent can, in itself, degrade the watermark, even without any attacks on the original sample. The inverse processes for Stable Diffusion and Open Sora, while slightly different, are both approximations and accumulate error. Consider a watermarked image $X$, an attacked image $Y$ and the predicted noise latent $f(Y)$. We can view the errors introduced by $f$ as simply additional image attacks. Then $X-Y-f(Y)$ form a Markov Chain. By the Data Processing Inequality, $I(X,f(Y))\leq I(X, Y)$. It follows that the capacity of a watermark extracted from the approximate inverted latent representation $f(Y)$ is upper bounded by the capacity of a watermark extracted directly from $Y$. Thus, if $f$ is not a bijection, we will have lower watermark capacity by embedding it into the initial Gaussian latent. Even still, generative watermarking is appealing because the initial latent Gaussian distribution creates a great opportunity for elegant schemes with much less impact on visual quality and yet also more robustness than many common ad-hoc methods.

\input{prc-overview}

\section{Experiments}
\subsection{Experiment Setup}
Our experimental design is divided into two phases: spatial robustness and temporal robustness. The first phase establishes a baseline by evaluating the PRC watermark's robustness against spatial desynchronization attacks on images generated by Stable Diffusion 1.5. The second phase, which forms the core of this work, extends the PRC watermark to the temporal domain using Open Sora 1.3 and rigorously evaluates its resilience against video-specific desynchronization attacks.



To build intuition for the video domain, we first examine the analogous effects of spatial desynchronization—specifically, cropping and rotation—on watermarked images generated by the Stable Diffusion 1.5 text-to-image model. While these geometric attacks were not discussed in the original PRC paper \citep{gunn2025undetectable}, they serve as critical benchmarks in the broader watermarking literature \citep{wen2023treeringwatermarksfingerprintsdiffusion} and are a necessary precursor to our analysis of video.

\subsection{Stable Diffusion 1.5}
While PRC was originally proposed and evaluated without geometric attacks \citep{gunn2025undetectable}, spatial perturbations such as cropping and rotation are standard robustness benchmarks in classical and generative watermarking \citep{wen2023treeringwatermarksfingerprintsdiffusion}. These attacks serve as direct spatial analogs to temporal clipping and frame dropping in videos.

\paragraph{Attack Setup}
We evaluate masked cropping and rotation attacks that simulate common post-processing operations. For cropping, removed regions are replaced with black pixels while preserving the original image dimensions, isolating the effect of spatial deletion without introducing rescaling artifacts.

\begin{table}[h]
    \centering
    \begin{threeparttable}
        \begin{tabular}{lcc}
            \toprule
            \textbf{Spatial Attack} & \textbf{Decodable Limit} & \textbf{Detectable Limit} \\
            \midrule
            Center crop (masked) & $< 30\%$ removed & $< 50\%$ removed \\
            Corner crop (masked) & $< 25\%$ removed & $< 40\%$ removed \\
            Rotation ($90^\circ$, $270^\circ$) & \multicolumn{2}{c}{\textit{All fail}} \\
            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \small
            \item “Removed” denotes the percentage of total image area occluded.
        \end{tablenotes}
    \end{threeparttable}
    \caption{\textbf{Spatial Robustness Limits for Images (Stable Diffusion 1.5).} Empirical breakdown points for PRC under spatial desynchronization.}
    \label{tab:image_spatial_limits}
\end{table}

\paragraph{Discussion}
The different cropping robustness exemplifies the PRC's error correction capability. However, the PRC fails for rotations. As we will discuss later, this is a result of spatial restructuring causing latent restructuring and thus a broken code. These image-based failure modes closely parallel the temporal desynchronization effects observed in the video domain, motivating our subsequent analysis of PRC under temporal attacks in Open Sora 1.3.

\subsection{Open Sora 1.3}
We extend the PRC watermark to the video domain using Open Sora 1.3. We approached this problem through three iterative implementations, moving from a naive theoretical extension to a more practical solution.

Crucially, Open Sora 1.3 operates on a compressed spatiotemporal latent space. The model compresses temporal data such that 1 latent frame corresponds to 4 real video frames. Consequently, our watermarking key $K$ exists in the dimensions $(W \times H \times T_{latent})$, while the attacks occur on the generated video in the pixel domain ($W_{pixel} \times H_{pixel} \times T_{real}$).

\subsubsection{Attempt 1: The Native Spatiotemporal Extension}
To start, we directly extended the PRC embedding scheme from the 2D spatial latent space ($W \times H$) used in image models to the full 3D spatiotemporal latent space ($W \times H \times T_{latent}$). To do this, we generate a unique watermarking key $K \in \mathbb{R}^{W \times H \times T_{latent}}$ that spans the entire volume of the generated video. 

\textbf{Imperceptibility Analysis.} Before evaluating robustness, however, we first established the imperceptibility of the watermark by evaluating it on VBench, a standard video generation benchmark that has been shown to align well with human preferences \citep{zheng2025vbench2}. This benchmark is made up of 2000 curated image prompts that are targeted towards 16 different metrics comprising a quality, semantic, and total score. For each prompt, we generate 5 watermarked and unwatermarked 2-second video clips, each at a resolution of 360p.

\begin{figure}[H]
    \centering
    \begin{minipage}[c]{0.58\linewidth} % 'c' centers vertically
        \centering
        \resizebox{\linewidth}{!}{ % scales table to minipage width
        \begin{tabular}{c|ccc}
            \hline
             Model & Total (\%) & Quality (\%) & Semantic (\%) \\
             \hline
             Open-Sora 1.3 & \textbf{78.43} & \textbf{81.57} & 65.89\\
             Open-Sora 1.3 w/ PRC & 78.32 & 81.40 & \textbf{66.04}  \\ 
        \end{tabular}
        }
    \end{minipage}
    \hfill
    \begin{minipage}[c]{0.38\linewidth} % 'c' centers vertically
        \centering
        \includegraphics[width=\linewidth]{images/vbench_metrics.png}
    \end{minipage}
    \caption{Weighted VBench summary statistics (left), Full VBench metric comparison with standard error bars (right).}
    \label{fig:vbench}
\end{figure}

The results in Figure \ref{fig:vbench} show that the PRC watermark did not significantly impact the generative ability of the Open Sora model. We conclude that, similar to the results shown in the original PRC paper, the distributions of the watermarked and unwatermarked Open-Sora samples are reasonably similar. 

\textbf{Robustness Failure.} While the visual quality was maintained, the temporal robustness proved to be a critical failure point. Because the watermark is embedded in the latent space, but attacks occur in the real frame space, even minor temporal disruptions are catastrophic. For example, dropping a single real frame shifts the entire sequence, misaligning the subsequent groups of 4 real frames with their corresponding latent key slice $K_t$. The results in Table \ref{tab:robustness_study} show that if we pad frames exactly where they were removed, thus keeping the original latent alignment, the scheme is still robust. This is comparable to a pixel-level attack on an image. If we don't know where frames were removed and instead pad arbitrarily, then the latent structure is completely reordered, just like an image rotation. Unsurprisingly, padding in incorrect positions makes the PRC undecodable.

\subsubsection{Attempt 2: Temporal Broadcasting}
To address this fragility, we adopted a "Temporal Broadcasting" strategy. Instead of a unique key for every latent frame, we generate a single spatial watermark $k \in \mathbb{R}^{W \times H}$ and repeat it $T_{latent}$ times, creating a spatiotemporally redundant signal where every latent frame carries an identical watermark.

In theory, this redundancy should render the watermark robust to any temporal cropping; since every frame carries the same watermark $k$, the detector should not require a specific temporal index to recover the signal. However, in practice, this approach fails significantly due to the decoupling of the attack domain (pixels) and the embedding domain (latents).

More specifically, the core issue lies in the 4:1 temporal compression ratio of the Open Sora 1.3 autoencoder. When the watermark is embedded, it is placed via latent frames, each of which is a compressed representation of 4 specific physical frames (e.g., Latent 0 represents Frames 0-3). Temporal attacks, such as clipping or frame dropping, operate on the physical frames. If an attack removes a number of frames that is not a multiple of 4 (e.g., removing the first frame), the alignment of the pixel groupings shifts. For instance, if we remove the first physical frame, the sequence [Frame 1, 2, 3, 4] is encoded instead, producing a latent vector completely distinct from the original watermarked latent generated from [Frame 0, 1, 2, 3]. Consequently, naive padding (e.g., appending black frames to the end) fails to correct this alignment, rendering the watermark undetectable under front-clipping or random frame dropping attacks.

Additionally, we observed that subsampling attacks (e.g., retaining every 4th frame) lead to a near-total collapse of detection. This is expected behavior: since the Open Sora 1.3 encoder requires 4 consecutive physical frames to construct a single valid latent representation, feeding it disjoint frames (e.g., frames 0, 4, 8, 12) deprives the encoder of 75\% of the required temporal information. The resulting latent vectors are heavily distorted, destroying the watermark signal.

Interestingly, we found that the first latent frame remained detectable even under severe subsampling. This suggests that while temporal aliasing destroys the watermark signal in subsequent latents, the initial latent frame acts as a sequence anchor and preserves enough spatial fidelity to allow for detection. This hypothesis is further supported by our finding that removing just the first physical frame (Frame 0) prior to subsampling eliminates detection entirely. This indicates that the watermark's survival is not merely a property of the first latent block, but is strictly contingent on the presence of the very first physical frame, likely due to the autoencoder's initialization.

\subsubsection{Attempt 2.5: Modulo-4 Alignment Strategy}
The failure of Attempt 2 (summarized in Table~\ref{tab:robustness_study}) clarified that simply repeating the key is insufficient; the phase of the real-to-latent compression also matters. Because Open Sora 1.3 encodes videos with a ratio of 4 real frames to 1 latent frame, a temporal shift/crop that is not a multiple of 4 disrupts the latent representation of the watermark. 

To solve this, we implemented the \textbf{Modulo-4 Alignment Strategy}. Instead of a single padding configuration, our detector evaluates four distinct temporal alignments. We shift the incoming real video frames by an offset $i \in \{0, 1, 2, 3\}$ before encoding them back into the latent space (and padding the remainder). By cycling through these four phase shifts, we ensure that at least one configuration correctly aligns the real frames to the latent used during generation.

This refined strategy successfully mitigates the synchronization errors observed in previous attempts. As shown in Table~\ref{tab:robustness_study}, Attempt~2.5 achieves broad robustness across all attack vectors where Attempt~2 failed. Recall, however, that temporal robustness required preserving the very first frame of the sample video. We further quantify the precise operating limits of our method in Table~\ref{tab:operating_limits}, demonstrating that the watermark remains reliably detectable even when over $55\%$ of the video content is removed or when frames are dropped with a probability of $p=0.75$.

% \begin{table}[h]
%     \centering
%     \begin{threeparttable}
%         \begin{tabular}{lcc}
%             \toprule
%             \textbf{Temporal Attack} & \textbf{Decodable Limit} & \textbf{Detectable Limit} \\
%             \midrule
%             \textbf{Boundary Clipping} & & \\
%             \hspace{3mm} \% Video Removed & $< 35\%$ removed & $< 55\%$ removed \\
%             \midrule
%             \textbf{Frame Dropping} & & \\
%             \hspace{3mm} Drop Probability ($p$) & $p < 0.75$ & $p < 0.75$ \\
%             \midrule
%             \textbf{Subsampling} & & \\
%             \hspace{3mm} Max Factor ($k$) & Tested up to $5\times$\textsuperscript{*} & Tested up to $5\times$\textsuperscript{*} \\
%             \bottomrule
%         \end{tabular}
%         \begin{tablenotes}
%             \small
%             \item \textsuperscript{*} See Table~\ref{tab:ablation_study} for architectural caveats regarding subsampling.
%         \end{tablenotes}
%     \end{threeparttable}
%     \caption{\textbf{Operating Limits of Attempt 2.5 (Modulo-4 Alignment).}}
% \end{table}

% \begin{table}[h]
%     \centering
%     \begin{threeparttable}
%         \begin{tabular}{lcccc}
%             \toprule
%             & \multicolumn{2}{c}{\textbf{Attempt 1: Native 3D}} & \textbf{Attempt 2} & \textbf{Attempt 2.5 (Ours)} \\
%             \cmidrule(lr){2-3} \cmidrule(lr){4-4} \cmidrule(lr){5-5}
%             \textbf{Attack Type} & \textit{Pad End} & \textit{Pad Front} & \textit{Pad End} & \textbf{Modulo-4 Align} \\
%             \midrule
%             \textbf{Boundary Clipping} & & & & \\
%             \hspace{3mm} Clip End (Back) & \cmark & \xmark & \cmark & \cmark \\
%             \hspace{3mm} Clip Start (Front) & \xmark & \cmark & \textbf{$\sim$}\textsuperscript{$\dagger$} & \cmark \\
%             \midrule
%             \textbf{Temporal Sampling} & & & & \\
%             \hspace{3mm} Frame Dropping & \xmark & \xmark & \cmark & \cmark \\
%             \hspace{3mm} Subsampling & \xmark & \xmark & \textbf{$\sim$}\textsuperscript{*} & \textbf{$\sim$}\textsuperscript{*} \\
%             \bottomrule
%         \end{tabular}
%         \begin{tablenotes}
%           \small
%           \item \cmark = Robust \quad \xmark = Fail \quad \textbf{$\sim$} = Conditional Success
%           \item \textsuperscript{$\dagger$} \textbf{Phase Mismatch:} Only detectable if $L \equiv T \pmod 4$
%           \item \textsuperscript{*} \textbf{Anchor Effect:} Detection confined to the first latent frame
%         \end{tablenotes}
%         \caption{\textbf{Robustness Ablation Study.}}
%         \label{tab:ablation_study}
%     \end{threeparttable}
% \end{table}

\begin{table}[h]
    \centering
    \begin{minipage}[t]{0.38\textwidth}
        \centering
        \begin{threeparttable}
            \footnotesize
            \caption{\textbf{Operating Limits}}
            \label{tab:operating_limits}
            \begin{tabular}{lccc}
                \toprule
                 & \multicolumn{2}{c}{\textbf{Limits}} \\
                \cmidrule(lr){2-3}
                \textbf{Attack Type} & \textbf{Decodable} & \textbf{Detectable} \\
                \midrule
                \textbf{Boundary Clip} & $< 35\%$ & $< 55\%$ \\
                \textbf{Frame Drop} & $p < 0.75$ & $p < 0.75$ \\
                \textbf{Subsampling} & $5\times$\tnote{*} & $5\times$\tnote{*} \\
                \bottomrule
            \end{tabular}
            \begin{tablenotes}
                \scriptsize
                \item[*] See Table III for caveats.
            \end{tablenotes}
        \end{threeparttable}
    \end{minipage}
    \hfill % Pushes the tables to the edges, but since they are wide, the gap is small
    % Right Table: Robustness (~60% of page width)
    \begin{minipage}[t]{0.60\textwidth}
        \centering
        \begin{threeparttable}
            \footnotesize
            \caption{\textbf{Robustness Study}}
            \label{tab:robustness_study}
            \setlength{\tabcolsep}{3pt} % Slight squeeze on column padding to fit full words
            \begin{tabular}{lcccc}
                \toprule
                 & \multicolumn{2}{c}{\textbf{Clipping}} & \multicolumn{2}{c}{\textbf{Temporal Sampling}} \\
                \cmidrule(lr){2-3} \cmidrule(lr){4-5}
                \textbf{Method} & \textbf{End} & \textbf{Front} & \textbf{Dropping} & \textbf{Subsampling} \\
                \midrule
                \textbf{Attempt 1: Native 3D} & & & & \\
                \hspace{3mm}\textit{Pad End} & \cmark & \xmark & \xmark & \xmark \\
                \hspace{3mm}\textit{Pad Front} & \xmark & \cmark & \xmark & \xmark \\
                \midrule
                \textbf{Attempt 2: Redundancy} & & & & \\
                \hspace{3mm}\textit{Pad End} & \cmark & \textbf{$\sim$}\tnote{$\dagger$} & \cmark & \textbf{$\sim$}\tnote{*} \\
                \midrule
                \textbf{Attempt 2.5} & & & & \\
                \hspace{3mm}\textbf{Modulo-4 Align} & \cmark & \cmark & \cmark & \textbf{$\sim$}\tnote{*} \\
                \bottomrule
            \end{tabular}
            \begin{tablenotes}
                \scriptsize
                \item \cmark = Robust, \xmark = Fail, \textbf{$\sim$} = Conditional Success
                \item[$\dagger$] Phase Mismatch: Only detectable if $L \equiv T \pmod 4$
                \item[*] Anchor Effect: Detection confined to first latent frame
            \end{tablenotes}
        \end{threeparttable}
    \end{minipage}
\end{table}

% \begin{table}[h]
% \centering
% \begin{threeparttable}
% \begin{tabular}{llccc}
% \toprule
% \textbf{Attack} & \textbf{Metric} & \textbf{Attempt 1} & \textbf{Attempt 2} & \textbf{Attempt 2.5} \\
% \midrule
% \multirow{2}{*}{Boundary Clipping}
% & Decodable Limit & \xmark & $\mathbf{\sim}^{\dagger}$ & $<35\%$ removed \\
% & Detectable Limit & \xmark & $\mathbf{\sim}^{\dagger}$ & $<55\%$ removed \\
% \midrule
% \multirow{2}{*}{Frame Dropping}
% & Decodable Limit & \xmark & \cmark & $p < 0.75$ \\
% & Detectable Limit & \xmark & \cmark & $p < 0.75$ \\
% \midrule
% \multirow{2}{*}{Subsampling}
% & Decodable Limit & \xmark & $\mathbf{\sim}^{*}$ & Tested up to $5\times^{*}$ \\
% & Detectable Limit & \xmark & $\mathbf{\sim}^{*}$ & Tested up to $5\times^{*}$ \\
% \bottomrule
% \end{tabular}
% \begin{tablenotes}
% \small
% \item \cmark = Robust \quad \xmark = Fail \quad $\mathbf{\sim}$ = Conditional Success
% \item[$\dagger$] \textbf{Phase Mismatch:} Only detectable if remaining frame count satisfies $L \equiv T \pmod{4}$.
% \item[$*$] \textbf{Anchor Effect:} Detection confined exclusively to the first latent frame.
% \end{tablenotes}
% \end{threeparttable}
% \caption{\textbf{Unified robustness summary under temporal attacks.}}
% \label{tab:temporal_unified}
% \end{table}

% \begin{itemize}
%     \item Clipping video to be shorter (beginning, end, middle, random frames)
%     \item Speed Manipulation (Sub-sampling or double-sampling)
%     \item swapping frames
%     \item reversing the video
%     \item Normal cropping of the video
%     \item Looping???
% \end{itemize}
% Human Eval Distinguishability (given 2 videos, 1 watermarked, 1 unwatermarked) pick the watermarked image (run across 10 videos 4 ppl, and avg)

\section{Discussion}
Our results demonstrate that spatial attacks on images, such as rotation and cropping, act as direct analogs to temporal attacks on videos. These shared failure modes stem from the VAE architectures employed by both model types. Because these architectures map spatial/temporal regions directly to corresponding latent regions, they lack the translation invariance required for PRC robustness. A rotation in pixel space or a frame drop in time completely offsets the latent grid, converting a simple synchronization shift into a catastrophic "deletion attack" on the watermark code. As established in prior work, PRC is robust to substitution but fails under deletion, as a shift at the beginning of the sequence destroys the alignment of the entire pseudorandom key \citep{gunn2025undetectable}.

\section{Future Directions}

\textbf{Exploiting the Anchor Frame Effect.} Our discovery that watermark detection under subsampling persists strictly in the first latent frame suggests that video VAEs heavily prioritize sequence initialization. Future work should analyze the internal workings to determine if this bias can be systematically exploited.

\textbf{Generalization to Emerging Architectures.} The synchronization challenges identified in Open Sora 1.3 likely extend to any latent video model with temporal compression. Future work should evaluate these techniques on state-of-the-art architectures like Wan2.1 to determine how causal latent structures affect the difficulty of embedding robust watermarks across frames.

\textbf{Expanding Capacity via Temporal Modulation.} While temporal broadcasting maximizes robustness through redundancy, it limits capacity. We believe that the temporal dimension can be used to achieve higher watermarking capacity without sacrificing on robustness. A potential scheme towards achieving this is injecting the PRC into the latent frequency domain. This approach could utilize the full temporal volume while maintaining invariance to the shift-based distortions identified in this work.

\section{Conclusion}
    As generative video models continue to advance, the ability to verify authenticity and identify AI-generated content is becoming increasingly critical. In this work, we extended the PRC watermark to the video domain, exposing a fundamental vulnerability in naive implementations: the synchronization mismatch between pixel-domain attacks and latent-domain compression. By introducing the Modulo-4 Alignment Strategy, we were able to improve robustness to severe temporal attacks despite this vulnerability. While this protocol significantly improves detection under desynchronization, it fails to take advantage of the higher capacity possible with video latents. More work is also needed to address the broader range of temporal distortions encountered in real-world pipelines and to develop watermarks that are inherently robust to these adversarial shifts.
\newpage

\bibliographystyle{plainnat}  
\bibliography{references}

\end{document}


%%%%%%
%% Some comments about useful packages
%% (extract from bare_conf.tex by Michael Shell)
%%

% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.


% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.


% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex


% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath


% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment for describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx


% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array

% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.


% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig


% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm's dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix


% *** PDF and URL PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.



% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
%%%%%%


%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End: